# Title: Addressing Bias and Copyright in Generative AI: Preference Matching and Fair Compensation

![Weijie Su](/assets/img/WeijieSu.jpg){: .img-fluid .rounded-circle .align-left  width="300px"}

**Speaker:** Weijie Su
**Date:** 10-06-2024, 2pm - 3pm (BST)
**Location:** Mathematical Sciences Building, MB0.08, University of Warwick, Coventry, UK

[Download iCalendar File](/assets/ics/event.ics)

## Abstract

Generative AI has demonstrated impressive capabilities in a range of data science and machine learning tasks, but their societal impact raises critical questions. This talk delves into two pressing issues: ensuring fairness in AI systems to adequately represent minority groups and addressing the challenges of using copyrighted data for training these models. We introduce Preference Matching (PM) RLHF, a new approach to mitigating algorithmic bias in reinforcement learning from human feedback, demonstrating improvements in aligning AI outputs with diverse human preferences. PM RLHF leverages a novel regularizer derived from an ordinary differential equation to balance response diversity and reward maximization. Additionally, we propose an economic framework that fairly compensates copyright owners based on their proportional input, leveraging probabilistic modeling and cooperative game theory. This framework fosters a sustainable and equitable data-sharing ecosystem, promoting mutual benefits for both AI developers and copyright owners through enhanced collaboration and fair compensation.

---

### About [Weijie Su](http://stat.wharton.upenn.edu/~suw/)

Generative AI has demonstrated impressive capabilities in a range of data science and machine learning tasks, but their societal impact raises critical questions. This talk delves into two pressing issues: ensuring fairness in AI systems to adequately represent minority groups and addressing the challenges of using copyrighted data for training these models. We introduce Preference Matching (PM) RLHF, a new approach to mitigating algorithmic bias in reinforcement learning from human feedback, demonstrating improvements in aligning AI outputs with diverse human preferences. PM RLHF leverages a novel regularizer derived from an ordinary differential equation to balance response diversity and reward maximization. Additionally, we propose an economic framework that fairly compensates copyright owners based on their proportional input, leveraging probabilistic modeling and cooperative game theory. This framework fosters a sustainable and equitable data-sharing ecosystem, promoting mutual benefits for both AI developers and copyright owners through enhanced collaboration and fair compensation.
