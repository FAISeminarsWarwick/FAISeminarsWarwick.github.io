<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="conservation-laws-for-gradient-flows">Conservation Laws for Gradient Flows</h1> <p><img src="/assets/img/gabriel_peyre.jpg" alt="Gabriel Peyre" class="img-fluid rounded-circle align-left"></p> <p><strong>Speaker:</strong> Gabriel Peyre (CNRS Researcher, DMA/ENS, France) <strong>Date:</strong> 2pm, June 14th, 2024 <strong>Location:</strong> MB0.07, University of Warwick, Coventry, UK</p> <p><a href="/assets/ics/event.ics">Download iCalendar File</a></p> <h2 id="abstract">Abstract</h2> <p>Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This “implicit bias” is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. In this talk I will first rigorously expose the definition and basic properties of “conservation laws”, which are maximal sets of independent quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then I will explain how to find the exact number of these quantities by performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. In the specific case of linear and ReLu networks, this procedure recovers the conservation laws known in the literature, and prove that there are no other laws. This is a joint work with Sibylle Marcotte and Rémi Gribonval. The associated paper can be found here: https://arxiv.org/pdf/2307.00144</p> <hr> </body></html>